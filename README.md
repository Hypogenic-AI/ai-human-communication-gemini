
# AI-to-Human Communication Effectiveness Study

This project investigates effective methods for AI systems to communicate dense information to human users.

## Project Overview

We explored whether structured communication formats (like hierarchical summaries or Q&A interfaces) are more effective for user comprehension than traditional, dense text summaries. Using a simulated user powered by Large Language Models (LLMs), we measured comprehension accuracy and user preference across these different formats.

The full research process and findings are detailed in [REPORT.md](REPORT.md).

## Key Findings

Our experiments yielded several key insights:

1.  **Dense Summaries Are Highly Effective:** The traditional dense summary format achieved the highest comprehension accuracy (80%) in our tests.
2.  **Q&A Interfaces Are a Strong Alternative:** A pre-computed Q&A format performed nearly as well as the dense summary (76% accuracy), making it a viable and efficient alternative.
3.  **Preference-Performance Paradox:** The "Hierarchical Summary" was the most preferred format by our simulated user, yet it resulted in the lowest comprehension accuracy (56%). This highlights a critical disconnect between what users prefer and what is most effective for a task.
4.  **Implication:** A "one-size-fits-all" approach to AI communication is suboptimal. The best format depends on the user's task. For detailed comprehension, dense text is effective. For quick lookups, a Q&A format excels. User preference alone is not a reliable metric of effectiveness.

## How to Reproduce the Experiment

### 1. Environment Setup

This project uses `uv` for environment management.

```bash
# Create the virtual environment
uv venv

# Install dependencies from the lock file or requirements
# Note: The original setup used 'uv pip install' due to a hatchling issue.
# The requirements.txt is provided for reproducibility.
uv pip install -r requirements.txt
```

### 2. Running the Experiment

The experiment is split into two main stages: artifact generation and evaluation.

**Stage 1: Generate Communication Artifacts**

First, you must have a valid `OPENAI_API_KEY` set as an environment variable. The script `src/prepare_data.py` will be run implicitly by the main experiment script if the data is not found, but it is recommended to run it first.

The `run_experiment.py` script originally performed both generation and evaluation. For clarity in reproduction, the generation step has been completed and the artifacts are saved. To re-generate them, you would need to adapt the script to re-run the generation functions. The core evaluation can be run directly.

**Stage 2: Run the Evaluation**

This script will load the pre-generated artifacts and run the full evaluation protocol.

```bash
# Ensure your OPENAI_API_KEY is set
export OPENAI_API_KEY="your-key-here"

# Run the evaluation
./.venv/bin/python src/run_experiment.py
```
This will produce `results/evaluation_results.json`.

### 3. Analyze the Results

Run the analysis script to generate a summary and plots from the evaluation results.

```bash
./.venv/bin/python src/analyze_results.py
```
This will output:
- `results/analysis_summary.txt`: A text file with mean accuracies and statistical test results.
- `results/accuracy_comparison.png`: A bar chart visualizing the comprehension accuracy of each method.

## File Structure

- `planning.md`: The initial research and experimental design plan.
- `REPORT.md`: The complete research report with detailed findings and analysis.
- `src/`: Contains the Python scripts for the experiment.
  - `prepare_data.py`: Loads and saves the initial dataset.
  - `run_experiment.py`: Main script to run the evaluation.
  - `analyze_results.py`: Script to analyze results and generate plots.
- `artifacts/`: Contains data artifacts used in the experiment.
  - `selected_articles.json`: The 5 articles selected for the study.
- `results/`: Contains the outputs of the experiment and analysis.
  - `generated_artifacts.json`: The communication artifacts generated by the LLM.
  - `evaluation_results.json`: The raw scores and preferences from the evaluation.
  - `analysis_summary.txt`: The final analysis report.
  - `accuracy_comparison.png`: The final plot.
- `requirements.txt`: A list of Python dependencies.
